{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state space is constructed based on the trajectory of price movements over a recent sequence of time points. At any particular moment $t$, the state, denoted $S_t$, is encapsulated by the vector of length $l$\n",
    "$$\n",
    "S_t=\\left[d_{t-l+1}, d_{t-l+2}, \\ldots, d_t\\right] \\text {, }\n",
    "$$\n",
    "where each $d_i$ characterizes the direction and magnitude of price changes at time $i$. A positive value of di signifies a price increase relative to time $i$ − 1, and conversely, a negative value indicates a decline. It is further determined by a relative change of price $\\pi_i$ by \n",
    "$$\n",
    "d_{\\mathrm{i}}= \\begin{cases}+1 & \\text { if } \\pi_{\\mathrm{i}}>k \\\\ +0 & \\text { if } 0<\\pi_{\\mathrm{i}}<k \\\\ -1 & \\text { if }-k<\\pi_{\\mathrm{i}}<0 \\\\ -2 & \\text { if } \\pi_{\\mathrm{i}}<-k\\end{cases}\n",
    "$$\n",
    "Therefore, +2 indicates a significant increase, while -1 a moderate decrease. $k$ is an adjustable sensitive level.    \n",
    "In this way we construct a state space of $4^l$ states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_relative_price_changes(price_data):\n",
    "    relative_changes = np.diff(price_data) / price_data[:-1]\n",
    "    return relative_changes\n",
    "\n",
    "def get_state(price_changes, k=0.03):\n",
    "    state = []\n",
    "    for change in price_changes:\n",
    "        if change > k:\n",
    "            state.append(1)\n",
    "        elif 0 < change <= k:\n",
    "            state.append(0)\n",
    "        elif -k <= change < 0:\n",
    "            state.append(-1)\n",
    "        else:\n",
    "            state.append(-2)\n",
    "    return tuple(state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space is composed of three possible actions: selling one share is represented by −1, taking no action is denoted by 0, and buying one share is indicated by +1.    \n",
    "If no position is held, selling action (-1) is not permissible, of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "????/???????/???????/???????/???????/???????/???????/???????/???"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The immediate reward, $R_{t+1}$, earned by the agent for taking action $A_t$ under prevailing environmental conditions, is mathematically defined as:\n",
    "$$\n",
    "R_{t+1}=A_t \\cdot\\left(\\theta-X_t\\right)-c \\cdot\\left|A_t\\right|,\n",
    "$$\n",
    "where $X_t$ denotes the current price of the spread, and $\\theta$ represents the true global mean of $X_t$, c is the transaction cost per trade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward(action, current_price, mean_price, transaction_cost):\n",
    "    return action * (mean_price - current_price) - transaction_cost * abs(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 292.29it/s]\n"
     ]
    }
   ],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, state_space_size, action_space_size, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.q_table = np.zeros((state_space_size, action_space_size))\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = [-1, 0, 1]  # sell, hold, buy\n",
    "\n",
    "    def encode_state(self, state, base=4):\n",
    "        state_int = 0\n",
    "        for i, value in enumerate(state):\n",
    "            state_int += (value + 2) * (base ** i)\n",
    "        return state_int\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state_int = self.encode_state(state)\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return np.random.choice(self.actions)  # Exploration\n",
    "        else:\n",
    "            return self.actions[np.argmax(self.q_table[state_int])]  # Exploitation\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        state_int = self.encode_state(state)\n",
    "        next_state_int = self.encode_state(next_state)\n",
    "\n",
    "        action_idx = self.actions.index(action)\n",
    "        best_next_action = np.max(self.q_table[next_state_int])\n",
    "        td_target = reward + self.gamma * best_next_action\n",
    "        td_error = td_target - self.q_table[state_int, action_idx]\n",
    "        self.q_table[state_int, action_idx] += self.alpha * td_error\n",
    "\n",
    "# Training the Q-learning agent\n",
    "def train_agent(price_data, mean_price, transaction_cost, episodes=1000, window_size=10):\n",
    "    state_space_size = 4 ** window_size\n",
    "    action_space_size = len([-1, 0, 1])\n",
    "    agent = QLearningAgent(state_space_size, action_space_size)\n",
    "\n",
    "    for episode in tqdm(range(episodes)):\n",
    "        for t in range(len(price_data) - window_size - 1):\n",
    "            price_window = price_data[t:t + window_size + 1]\n",
    "            next_price_window = price_data[t + 1:t + window_size + 2]\n",
    "            state = get_state(compute_relative_price_changes(price_window))\n",
    "            next_state = get_state(compute_relative_price_changes(next_price_window))\n",
    "\n",
    "            current_price = price_data[t + window_size - 1]\n",
    "            action = agent.choose_action(state)\n",
    "            reward = calculate_reward(action, current_price, mean_price, transaction_cost)\n",
    "\n",
    "            agent.update_q_table(state, action, reward, next_state)\n",
    "\n",
    "    return agent\n",
    "\n",
    "# Example usage with dummy price data\n",
    "price_data = np.random.randn(100)  # Replace with actual price data\n",
    "mean_price = np.mean(price_data)\n",
    "transaction_cost = 0.01\n",
    "agent = train_agent(price_data, mean_price, transaction_cost)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative choice is to model the Q-function as a neural network model. Below is an example. Notice we also simplify the reward function and do not limit action according to current position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the trading environment\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, prices, window_size):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.prices = prices\n",
    "        self.window_size = window_size\n",
    "        self.n_features = window_size\n",
    "        self.action_space = spaces.Discrete(3)  # 0: Sell, 1: Hold, 2: Buy\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.n_features,), dtype=np.float32)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = self.window_size\n",
    "        self.done = False\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        return self.prices[self.current_step - self.window_size:self.current_step]\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        self.current_step += 1\n",
    "\n",
    "        if self.current_step >= len(self.prices) - 150:\n",
    "            self.done = True\n",
    "\n",
    "        obs = self._next_observation()\n",
    "\n",
    "        if action == 0:  # Sell\n",
    "            reward = self._take_action(-1)\n",
    "        elif action == 2:  # Buy\n",
    "            reward = self._take_action(1)\n",
    "\n",
    "        return obs, reward, self.done, {}\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        prev_price = self.prices[self.current_step - 1]\n",
    "        curr_price = self.prices[self.current_step]\n",
    "        return action * (curr_price - prev_price)\n",
    "\n",
    "# Define the Q-learning agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.model = self._build_model()\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(self.state_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, self.action_size)\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def train(self, env, episodes=1000, gamma=0.95, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        for e in range(episodes):\n",
    "            state = env.reset()\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            for time in range(450):\n",
    "                if np.random.rand() <= epsilon:\n",
    "                    action = np.random.choice(env.action_space.n)\n",
    "                else:\n",
    "                    q_values = self.model(state)\n",
    "                    action = torch.argmax(q_values).item()\n",
    "\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "                target = reward\n",
    "                if not done:\n",
    "                    target = reward + gamma * torch.max(self.model(next_state)).item()\n",
    "                target_f = self.model(state)\n",
    "                target_f[0][action] = target\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.criterion(self.model(state), target_f)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                state = next_state\n",
    "                if done and e%10==0:\n",
    "                    print(f\"Episode: {e}/{episodes}, loss: {loss}, epsilon: {epsilon:.2}\")\n",
    "                    break\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon *= epsilon_decay\n",
    "\n",
    "# Load data\n",
    "def load_data(stock_symbol, start_date, end_date):\n",
    "    df = pd.read_csv(f\"{stock_symbol}.csv\", index_col='Date', parse_dates=True)\n",
    "    return df['Adj Close'].loc[start_date:end_date].values\n",
    "\n",
    "# Training the model\n",
    "if __name__ == \"__main__\":\n",
    "    stock_symbol = 'AAPL'  # Example stock\n",
    "    start_date = '2021-01-01'\n",
    "    end_date = '2022-12-31'\n",
    "    window_size = 10\n",
    "\n",
    "    prices = load_data(stock_symbol, start_date, end_date)\n",
    "    env = TradingEnv(prices, window_size)\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = QLearningAgent(state_size, action_size)\n",
    "    agent.train(env, episodes=200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poisson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
